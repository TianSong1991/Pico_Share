{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>损失函数</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\"> **损失函数**用来评价模型的**预测值**和**真实值**不一样的程度，损失函数越好，通常模型的性能越好。不同的模型用的损失函数一般也不一样。通常也成为代价函数。也称误差函数与代价函数。\n",
    "<br>\n",
    "<br>\n",
    "**损失函数**分为**经验风险损失函数**和**结构风险损失函数**。**经验风险损失函数**指预测结果和实际结果的差别，**结构风险损失函数**是指经验风险损失函数加上正则项。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0-1损失函数(zero-one loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\"> 0-1损失是指预测值和目标值不相等为1， 否则为0:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.zhihu.com/equation?tex=L+%28+Y+%2C+f+%28+X+%29+%29+%3D+%5Cleft%5C%7B+%5Cbegin%7Barray%7D+%7B+l+%7D+%7B+1+%2C+Y+%5Cneq+f+%28+X+%29+%7D+%5C%5C+%7B+0+%2C+Y+%3D+f+%28+X+%29+%7D+%5Cend%7Barray%7D+%5Cright.+++%5C%5C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\">\n",
    "特点：\n",
    "\n",
    "- 0-1损失函数直接对应分类判断错误的个数，但是它是一个非凸函数，不太适用.\n",
    "\n",
    "- 感知机就是用的这种损失函数。但是相等这个条件太过严格，因此可以放宽条件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.zhihu.com/equation?tex=L+%28+Y+%2C+f+%28+X+%29+%29+%3D+%5Cleft%5C%7B+%5Cbegin%7Barray%7D+%7B+l+%7D+%7B+1+%2C+%7C+Y+-+f+%28+X+%29+%7C+%5Cgeq+T+%7D+%5C%5C+%7B+0+%2C+%7C+Y+%3D+f+%28+X+%29+%7C+%3C+T+%7D+%5Cend%7Barray%7D+%5Cright.++%5C%5C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 绝对值损失函数-MAE-L1 loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\"> 绝对值损失函数是计算预测值与目标值的差的绝对值："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.zhihu.com/equation?tex=L%28Y%2C+f%28x%29%29+%3D+%7CY+-+f%28x%29%7C++%5C%5C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 平方损失函数-MSE-L2 loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.zhihu.com/equation?tex=L+%28+Y+%7C+f+%28+X+%29+%29+%3D+%5Csum+_+%7B+N+%7D+%28+Y+-+f+%28+X+%29+%29+%5E+%7B+2+%7D++%5C%5C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\"> 经常应用与回归问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smooth L1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img-blog.csdn.net/20171220152219736?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSk5pbmdXZWk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\"> 解决L1 loss的梯度不平滑和L2 loss的梯度爆炸。\n",
    "    \n",
    "faster rcnn框架中，使用了smooth L1 loss\n",
    "\n",
    "- 相比于L1损失函数，可以收敛得更快。\n",
    "- 相比于L2损失函数，对离群点、异常值不敏感，梯度变化相对更小，训练时不容易跑飞。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huber loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{split}\n",
    "L_\\delta(y, f(x))=\\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\frac12(y-f(x))^2,&\\textrm{for }|y-f(x)|\\leq\\delta\\\\\n",
    "\\delta\\cdot(|y-f(x)|-\\frac12\\delta),&\\textrm{otherwise.}\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\">Huber对于离群点非常的有效，它同时结合了L1与L2的优点，不过多出来了一个delta参数需要进行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对数损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.zhihu.com/equation?tex=L%28Y%2C+P%28Y%7CX%29%29+%3D+-logP%28Y%7CX%29++%5C%5C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\">\n",
    "- log对数损失函数能非常好的表征概率分布，在很多场景尤其是多分类，如果需要知道结果属于每个类别的置信度，那它非常适合。\n",
    "\n",
    "- 健壮性不强，相比于hinge loss对噪声更敏感。\n",
    "\n",
    "- 逻辑回归的损失函数就是log对数损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 指数损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.zhihu.com/equation?tex=L%28Y%7Cf%28X%29%29+%3D+exp%5B-yf%28x%29%5D++%5C%5C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\">对离群点、噪声非常敏感。经常用在AdaBoost算法中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hinge 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.zhihu.com/equation?tex=L%28y%2C+f%28x%29%29+%3D+max%280%2C+1-yf%28x%29%29+++%5C%5C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\">\n",
    "- hinge损失函数表示如果被分类正确，损失为0，否则损失就为1-yf(x)。SVM就是使用这个损失函数。\n",
    "- 一般的f(x)是预测值，在-1到1之间，y是目标值(-1或1)。其含义是，f(x)的值在-1和+1之间就可以了，并不鼓励|f(x)|>1 ，即并不鼓励分类器过度自信，让某个正确分类的样本距离分割线超过1并不会有任何奖励，从而使分类器可以更专注于整体的误差。\n",
    "- 健壮性相对较高，对异常点、噪声不敏感，但它没太好的概率解释。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉熵损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.zhihu.com/equation?tex=C+%3D+-+%5Cfrac+%7B+1+%7D+%7B+n+%7D+%5Csum+_+%7B+x+%7D+%5B+y+%5Cln+a+%2B+%28+1+-+y+%29+%5Cln+%28+1+-+a+%29+%5D++%5C%5C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\">\n",
    "(1)本质上也是一种对数似然函数，可用于二分类和多分类任务中。\n",
    "\n",
    "(2)当使用sigmoid作为激活函数的时候，常用交叉熵损失函数而不用均方误差损失函数，因为它可以完美解决平方损失函数权重更新过慢的问题，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>激活函数</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\">激活函数（activation function）又称非线性映射函数或是隐藏单元，是神经网络中中最主要的组成部分之一。如果没有激活函数，神经网络就是线性回归模型。激活函数的发展经历了Sigmoid -> Tanh -> ReLU -> Leaky ReLU -> Maxout这样的过程，还有一个特殊的激活函数Softmax，因为它只会被用在网络中的最后一层，用来进行最后的分类和归一化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sigmoid "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.zhihu.com/equation?tex=%5Csigma%28z%29%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-z%7D%7D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.zhihu.com/equation?tex=%5Csigma%5E%7B%E2%80%B2%7D%28z%29%3D%5Cfrac%7B0-1%C2%B7%28-e%5E%7B-z%7D%29%7D%7B%281%2Be%5E%7B-z%7D%29%5E%7B2%7D%7D%3D%5Cfrac%7Be%5E%7B-z%7D%7D%7B%281%2Be%5E%7B-z%7D%29%5E%7B2%7D%7D%3D%5Cfrac%7Be%5E%7B-z%7D%7D%7B%281%2Be%5E%7B-z%7D%29%7D%C2%B7%5Cfrac%7B1%7D%7B%281%2Be%5E%7B-z%7D%29%7D%3D%5Cfrac%7B1%2Be%5E%7B-z%7D-1%7D%7B%281%2Be%5E%7B-z%7D%29%7D%C2%B7%5Csigma%28z%29%3D%281-%5Csigma%28z%29%29%C2%B7%5Csigma%28z%29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pic2.zhimg.com/80/v2-595feb9c4660fdee432dcd30b8256735_hd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\">\n",
    "优点：平滑、易于求导。\n",
    "\n",
    "缺点：\n",
    "\n",
    "- 激活函数计算量大（在正向传播和反向传播中都包含幂运算和除法）；\n",
    "- 反向传播求误差梯度时，求导涉及除法；\n",
    "- Sigmoid导数取值范围是[0, 0.25]，由于神经网络反向传播时的“链式反应”，很容易就会出现梯度消失的情况。例如对于一个10层的网络，第10层的误差相对第一层卷积的参数w的梯度将是一个非常小的值，这就是所谓的“梯度消失”。\n",
    "- Sigmoid的输出不是0均值（即zero-centered）；这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入，随着网络的加深，会改变数据的原始分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.zhihu.com/equation?tex=tanh%28x%29+%3D+%5Cdfrac+%7Be%5Ex+-+e%5E%7B-x%7D%7D%7Be%5Ex+%2B+e%5E%7B-x%7D%7D+%5C%5C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pic1.zhimg.com/80/v2-ac5875cf045fbdf213b8b0ba67f10b30_hd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\">\n",
    "- tanh的输出范围时(-1, 1)，解决了Sigmoid函数的不是zero-centered输出问题；\n",
    "- 幂运算的问题仍然存在；\n",
    "- tanh导数范围在(0, 1)之间，相比sigmoid的(0, 0.25)，梯度消失（gradient vanishing）问题会得到缓解，但仍然还会存在。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\"> Relu(Rectified Linear Unit)——修正线性单元函数：该函数形式比较简单，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://gss0.bdstatic.com/94o3dSag_xI4khGkpoWK1HF6hhy/baike/pic/item/9d82d158ccbf6c818097871ab03eb13532fa409b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pic2.zhimg.com/80/v2-da3babf705f525effbaab3bbbed7df51_hd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\">\n",
    "- 相比Sigmoid和tanh，ReLU摒弃了复杂的计算，提高了运算速度。\n",
    "- 解决了梯度消失问题，收敛速度快于Sigmoid和tanh函数，但要防范ReLU的梯度爆炸\n",
    "- 容易得到更好的模型，但也要防止训练中出现模型‘Dead’情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky ReLU, PReLU（Parametric Relu）, RReLU（Random ReLU）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\">\n",
    "为了防止模型的‘Dead’情况，后人将x<0部分并没有直接置为0，而是给了一个很小的负数梯度值α。\n",
    "\n",
    "**Leaky ReLU**中的α为常数，一般设置 0.01。这个函数通常比 Relu 激活函数效果要好，但是效果不是很稳定，所以在实际中 Leaky ReLu 使用的并不多。\n",
    "<br>\n",
    "**PRelu（参数化修正线性单元）**中的α作为一个可学习的参数，会在训练的过程中进行更新。\n",
    "<br>\n",
    "**RReLU（随机纠正线性单元）**也是Leaky ReLU的一个变体。在RReLU中，负值的斜率在训练中是随机的，在之后的测试中就变成了固定的了。RReLU的亮点在于，在训练环节中，α是从一个均匀的分布U(I,u)中随机抽取的数值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pic3.zhimg.com/80/v2-96c799e1b9f0b80de1ca17e503e4c11a_hd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload-images.jianshu.io/upload_images/3589698-cd4f2280696b51ff.png?imageMogr2/auto-orient/strip|imageView2/2/w/250/format/webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload-images.jianshu.io/upload_images/3589698-302fd0a2a1dcbbac.png?imageMogr2/auto-orient/strip|imageView2/2/w/207/format/webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\">ELU函数是ReLU函数的改进型，相比于ReLU函数，在输入为负数时，有的输出，输出有一定抗干扰能力。消除ReLU死掉的问题，还是有梯度饱和和指数运算的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maxout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.zhihu.com/equation?tex=f%28x%29+%3D+max%28w%5ET_1x%2Bb_1%2Cw%5ET_2x%2Bb_2%29%5C%5C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pic1.zhimg.com/80/v2-690b93748aa2ad63f3939760f7207aa0_hd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\">maxout的拟合能力是非常强的，它可以拟合任意的的凸函数。最直观的解释就是任意的凸函数都可以由分段线性函数以任意精度拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\">\n",
    "- Maxout的拟合能力非常强，可以拟合任意的凸函数。\n",
    "- Maxout具有ReLU的所有优点，线性、不饱和性。\n",
    "- 同时没有ReLU的一些缺点。如：神经元的死亡。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\">\n",
    "    Softmax用于多分类神经网络输出，目的是让大的更大。函数公式是"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.zhihu.com/equation?tex=%5Csigma%28z%29_%7Bj%7D%3D%5Cfrac%7Be%5E%7Bz_%7Bj%7D%7D%7D%7B%5Csum_%7Bk%3D1%7D%5E%7BK%7D%7Be%5E%7Bz_%7Bk%7D%7D%7D%7D%5C%5C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pic1.zhimg.com/80/v2-68a7dfdf613d8cd43f0569184b206c5c_hd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\">Softmax是Sigmoid的扩展，当类别数k＝2时，Softmax回归退化为Logistic回归。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>优化函数</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\">优化函数：优化损失函数使模型快速收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\">深度学习优化算法经历了 SGD -> SGDM -> NAG ->AdaGrad -> AdaDelta -> Adam -> Nadam 这样的发展历程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pic1.zhimg.com/80/v2-c726db927d17180330071c52d6f541a8_hd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pic3.zhimg.com/80/v2-18df00ed2b351b531623715edadb1a6a_hd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.zhihu.com/equation?tex=%5Ctheta_i%5Cleftarrow%5Ctheta_i-%5Calpha%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%5Ctheta_i%7DJ%28%5Ctheta_0%2C%5Ctheta_1%2C...%2C%5Ctheta_n%29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://5b0988e595225.cdn.sohucs.com/images/20180820/055d35aae57544908351c24522017893.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://5b0988e595225.cdn.sohucs.com/images/20180820/199668ea6e6744e193b0ccde6e119d33.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 动量法momentum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\"> 动量法是通过指数加权平均使得各个方向的梯度尽可能的保持一致，减少梯度在各个方向的发散"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pic1.zhimg.com/80/v2-343e40136f6e80fea93f252981a560ec_hd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov动量SGD算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\"> Nesterov动量SGD算法与标准动量算法类似，只不过在计算梯度前对参数$\\theta$进行了校正。使得在计算梯度之前给参数$\\theta$加上一个动量因子，而不是原模原样地把上一次的$\\theta$拿来求解梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img-blog.csdn.net/20170521225135450?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvQlZMMTAxMDExMTE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\"> Adagrad算法是根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题。AdaGrad与SGD框架非常像！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pic2.zhimg.com/80/v2-386e1f3eefd768c25ef6a79d547c98a1_hd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://math.jianshu.com/math?formula=%5Cbegin%7Baligned%7D%20%26r%20%5Cleftarrow%20r%2Bg%20%5Codot%20g%5C%5C%20%26%5CDelta%20%5Ctheta%20%5Cleftarrow-%5Cfrac%7B%5Cepsilon%7D%7B%5Cdelta%2B%5Csqrt%7Br%7D%7D%5Codot%20g%5C%5C%20%26%5Ctheta%5Cleftarrow%20%5Ctheta%2B%5CDelta%20%5Ctheta%20%5Cend%7Baligned%7D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaDelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\"> RMSProp算法实际上对AdaGrad算法做出了改进，修改了累积平方梯度和参数更新量的计算方法，在非凸情况下比AdaGrad效果较好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pic3.zhimg.com/80/v2-b0294a1922789b3e99d1c8445c69dbba_hd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\">Adam算法与其他的优化算法相比，区别还是比较大的，在参数更新的过程中，首先计算了有偏一阶矩和二阶矩，然后修正一阶矩和二阶矩的偏差，再用修正后的一阶和二阶矩求解参数更新量，最后利用参数更新量对参数进行更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images2018.cnblogs.com/blog/1285295/201807/1285295-20180727143906845-518663688.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 face=\"微软雅黑\">Adam 算法同时获得了 AdaGrad 和 RMSProp 算法的优点。Adam 不仅如 RMSProp 算法那样基于一阶矩均值计算适应性参数学习率，它同时还充分利用了梯度的二阶矩均值（即有偏方差/uncentered variance）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pic1.zhimg.com/v2-5d5166a3d3712e7c03af74b1ccacbeac_b.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://5b0988e595225.cdn.sohucs.com/images/20180820/a9a31384a2404323a33c0a15b3c5721e.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
